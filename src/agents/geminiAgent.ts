import { GoogleGenerativeAI } from "@google/generative-ai";
import dotenv from "dotenv";
import { CacheManager } from "../utils/cacheManager";
import { AGENT_INSTRUCTIONS } from "../config/agentInstructions";
import { AgentResponse, TokenUsage } from "../config/agentConfig";
import path from 'path';

// Carrega as vari√°veis de ambiente
dotenv.config({ path: path.join(process.cwd(), '.env') });

// Log para debug das vari√°veis de ambiente
console.log('üîç Verificando vari√°veis de ambiente:');
console.log('GEMINI_API_KEY:', process.env.GEMINI_API_KEY ? '‚úÖ Configurado' : '‚ùå N√£o configurado');
console.log('Diret√≥rio atual:', process.cwd());
console.log('Caminho do .env:', path.join(process.cwd(), '.env'));

const API_KEY = process.env.GEMINI_API_KEY;
console.log('üîë API Key configurada:', API_KEY ? '‚úÖ Sim' : '‚ùå N√£o');

if (!API_KEY) {
  throw new Error("‚ùå API KEY do Gemini n√£o encontrada no .env (GEMINI_API_KEY)");
}

const client = new GoogleGenerativeAI(API_KEY);
const MAX_INPUT_LENGTH = 8000;
const cacheManager = new CacheManager();

/**
 * Gera uma explica√ß√£o para um log de erro usando a IA Gemini.
 * @param logContent Conte√∫do bruto do log.
 * @returns Resposta com conte√∫do e metadados de uso.
 */
export async function gerarExplicacaoErro(logContent: string): Promise<AgentResponse> {
  const startTime = Date.now();
  try {
    // Verifica cache
    const cachedResponse = await cacheManager.get<AgentResponse>(logContent);
    if (cachedResponse && cachedResponse.content) {
      console.log('üì¶ Usando resposta em cache');
      // Garante que a resposta do cache tenha todas as propriedades necess√°rias
      const response: AgentResponse = {
        content: cachedResponse.content,
        tokenUsage: cachedResponse.tokenUsage || {
          promptTokens: Math.ceil(cachedResponse.content.length / 4),
          completionTokens: Math.ceil(cachedResponse.content.length / 4),
          totalTokens: Math.ceil(cachedResponse.content.length / 2),
          timestamp: new Date().toISOString(),
          model: "gemini-1.5-flash"
        },
        processingTime: Date.now() - startTime
      };
      return response;
    }

    console.log('ü§ñ Gerando nova resposta com Gemini...');
    const sanitizedInput = logContent.length > MAX_INPUT_LENGTH
      ? logContent.substring(0, MAX_INPUT_LENGTH)
      : logContent;

    const model = client.getGenerativeModel({ 
      model: "gemini-1.5-flash",
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 1024,
        topP: 0.8,
        topK: 40
      }
    });

    const prompt = `
${AGENT_INSTRUCTIONS.role}

Por favor, analise o seguinte log de erro seguindo estas diretrizes:
${AGENT_INSTRUCTIONS.analysisGuidelines.map(g => `- ${g}`).join('\n')}

Formate sua resposta nas seguintes se√ß√µes:
${AGENT_INSTRUCTIONS.responseFormat.sections.map(s => `## ${s}`).join('\n')}

Log para an√°lise:
${sanitizedInput}
`;

    console.log('üìù Enviando prompt para o Gemini...');
    const result = await model.generateContent(prompt);
    const text = result.response.text().trim();

    if (!text) {
      throw new Error("Resposta da IA vazia.");
    }

    // Calcula uso de tokens (estimativa aproximada)
    const estimatedTokens = Math.ceil(text.length / 4); // Estimativa aproximada: 1 token ~ 4 caracteres
    const tokenUsage: TokenUsage = {
      promptTokens: estimatedTokens,
      completionTokens: estimatedTokens,
      totalTokens: estimatedTokens * 2,
      timestamp: new Date().toISOString(),
      model: "gemini-1.5-flash"
    };

    const response: AgentResponse = {
      content: text,
      tokenUsage,
      processingTime: Date.now() - startTime
    };

    console.log('üíæ Salvando resposta no cache...');
    // Salva no cache
    await cacheManager.set(logContent, response);

    return response;
  } catch (error: any) {
    console.error("‚ùå Erro detalhado:", error);
    if (error.details && error.details.error_message) {
      throw new Error(`Erro de Gemini: ${error.details.error_message}`);
    } else if (error.response) {
      throw new Error(`Erro de resposta Gemini: ${error.response.status}`);
    } else {
      throw new Error("Erro ao gerar explica√ß√£o com IA. Verifique sua conex√£o ou chave da API.");
    }
  }
}

// Exemplo de uso
async function executar() {
  try {
    const logErro = (`
    Algum erro aqui:
    Erro de conex√£o com o banco de dados.
    Detalhes: ERRO 1001
    Dados passados para a consulta: {nome: "Jo√£o", idade: 30}
    ...mais detalhes...`);

    const response = await gerarExplicacaoErro(logErro);
    console.log("Resposta:", response.content);
    console.log("\nüìä M√©tricas de uso:");
    console.log(`- Tokens do prompt: ${response.tokenUsage.promptTokens}`);
    console.log(`- Tokens da resposta: ${response.tokenUsage.completionTokens}`);
    console.log(`- Total de tokens: ${response.tokenUsage.totalTokens}`);
    console.log(`- Tempo de processamento: ${response.processingTime}ms`);
  } catch (error) {
    console.error("Erro ao executar:", error);
  }
}

executar();